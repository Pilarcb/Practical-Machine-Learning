
# PRACTICAL MACHINE LEARNING PROJECT: "PREDICTING HUMAN EXERCISE USING SELF-MONITORING DEVICES"

Pilar Cantero
19 de septiembre de 2016


## EXECUTIVE SUMMARY

The emergence of the digital age has been impacted with several technological changes where people are willing to measure 
their own individual daily activities related to work, exercise, sleep, diet, mood, etc. We are covering ourselves up with 
these new "gadgets", such as Fitbit, Jawbone Up and Nike FuelBand, which are collecting all this information.  These type 
of devices are part of the quantified self movement and one thing that people regularly do is quantify how much of a 
particular activity they do, but they rarely quantify how well they do it. 

In our analysis we carried out an experiment with a group of 6 participants (aged between 20-28 years) using data from 
accelerometers on the belt, forearm, arm and dumbbell, to build a model to predict the manner in which these participants 
did the exercise, and then to predict the movement of 20 different test cases. 

They were asked to perform barbell lifts correctly and incorrectly in 5 different ways: exactly according to the 
specification (Class A), throwing the elbows to the front (Class B), lifting the dumbbell only halfway (Class C), lowering
the dumbbell only halfway (Class D) and throwing the hips to the front (Class E). Class A corresponds to the specified 
execution of the exercise, while the other 4 classes correspond to common mistakes.

More information is available from the website here: http://groupware.les.inf.puc-rio.br/har (see the section on the Weight
Lifting Exercise Dataset).

Thus, it is an interesting problem to build a model that predicts what kind of exercise  a subject is performing 
based on the quantitative measurements from self monitoring devices.

Our analysis suggests that our prediction function, developed using the Random Forests method, will have a great
accuracy (over 99.70%) to predict the 20 test cases with 100% accuracy.


## BASIC SETTING 

RStudio

knitr

echo = TRUE

set.seed(12345)

Load libraries:

library(caret)
library(rpart)
library(rpart.plot)
library(rattle)
library(RColorBrewer)
library(randomForest)
library("e1071")
library(gbm)
library(ggplot2)
library(gridExtra).


## GETTING AND CLEANING DATA

The data for this project come from this original source: http://groupware.les.inf.puc-rio.br/har

Velloso, E.; Bulling, A.; Gellersen, H.; Ugulino, W.; Fuks, H. Qualitative Activity Recognition of Weight Lifting Exercises. Proceedings of 4th International Conference in Cooperation with SIGCHI (Augmented Human '13) . Stuttgart, Germany: ACM SIGCHI, 2013.

I would like to thank the authors for being very generous in allowing their data to be used for this kind of assignment.

The training data for this project are available here:
https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv

The test data are available here:
https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv

### Downloading and reading the data:
```{r}
trainurl <- "http://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv"
traindata <- read.csv(url(trainurl), na.strings=c("NA","#DIV/0!",""))
dim(traindata)
```
```{r}
testurl <- "http://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv"
validationdata <- read.csv(url(testurl), na.strings=c("NA","#DIV/0!",""))
dim(validationdata)
```
### Getting a subtraining data set and a subtesting data set from the original training data set to be used for Cross 
Validation: 

Dividing the original traindata set into two subdata sets: 60% in the finaltrain data set and 40% in the finaltest data set.
I will perform cross validation within the training division in order to improve the model fit. After that, 
I will do out-of-sample test with the testing division to validate the model where an expected out-of-sample error rate
of less than 0.5%, or 99.5% accuracy, would be acceptable before it is used to perform the prediction on the 20 test cases
(that must have 100% accuracy to obtain 20 points awarded). Therefore, I leave the original test set (validationdata) alone,
and I will apply our ultimate prediction algorithm to this test set in order to be an unbiased measurement.
```{r}
library (caret)
subtrain <- createDataPartition(traindata$classe, p=0.6, list=FALSE)
finaltrain <- traindata[subtrain, ]
finaltest<- traindata[-subtrain, ]
dim(finaltrain)
dim(finaltest)
```
###Cleaning data

I will have a look at these subdata sets and I will call the nearZeroVar function with the argument saveMetrics = TRUE
```{r}
x = nearZeroVar(finaltrain, saveMetrics = TRUE)
 str(x, vec.len=2)
y = nearZeroVar(finaltest, saveMetrics = TRUE)
str(y, vec.len=2)
```
By default, a predictor is classified as near-zero variance if the percentage of unique values in the samples is less 
than 10% and when the frequency ratio mentioned above is greater than 19 (95/5). 

We can explore which ones are the zero variance predictors:
```{r}
x[x[,"zeroVar"] > 0, ]
y[y[,"zeroVar"] > 0, ]
```
and which ones are the near-zero variance predictors:
```{r}
x[x[,"zeroVar"] + x[,"nzv"] > 0, ]
y[y[,"zeroVar"] + y[,"nzv"] > 0, ]
```
####1.- I will remove variables with nzv:
```{r}
finalnzvtrain <-finaltrain[, -nearZeroVar(finaltrain)]
dim(finalnzvtrain)
finalnzvtest <-finaltest[, -nearZeroVar(finaltest)]
dim(finalnzvtest)
```
####2.- In both data sets (finalnzvtrain and finalnzvtest) there are a lot of NA´s. I will remove variables that are
mostly NA´s:
```{r}
trainNA <- sapply(finalnzvtrain, function(x) mean(is.na(x))) > 0.95
NoNAtrain <-finalnzvtrain[, trainNA==FALSE]
dim(NoNAtrain)
testNA <- sapply(finalnzvtest, function(x) mean(is.na(x))) > 0.95
NoNAtest <-finalnzvtest[, testNA==FALSE]
dim(NoNAtest)
```
####3.- Having a look at the NoNAtrain and NoNAtest names, I will remove the columns (1:5) which seems to be 
identification variables.
```{r}
trainclean<-NoNAtrain[, -(1:5)]
testclean<-NoNAtest[, -(1:5)]
dim(trainclean)
dim(testclean)
````
After performing the cleaning data process, we got two data subsets of 54 variables each.

####4.- Processing validationdata and testclean data sets:
```{r}
clean1 <- colnames(trainclean)
clean2 <- colnames(trainclean[, -54])  # remove the classe column
testclean2 <- testclean[clean1]       # allow only variables in testclean that are                                                 #also in trainclean
validation2 <- validationdata[clean2]   # allow only variables in validationdata that are also in trainclean 
```
```{r}
dim(testclean2)
dim(validation2)
```
####5.- Coerce the data into the same type:
```{r}
for (i in 1:length(validation2) ) {
    for(p in 1:length(trainclean)) {
        if( length( grep(names(trainclean[i]), names(validation2)[p]) ) == 1)  {
            class(validation2[p]) <- class(trainclean[i])
        }      
    }      
}
```

####6.- Getting the same class between validation2 and trainclean:
```{r}
validation3 <- rbind(trainclean[2, -54] , validation2)
validationf <- validation3[-1,]
```
## PREDICTION MODEL BUILDING

I will use three methods in the training data set (trainclean) to model the regressions and which one that is more 
accurate, I will apply to the testing set (validationf) and use it  for the quiz prediction. These methods are: 
Decision Trees, Random Forests, and Generalized Boosted Model. Also, I will plot a Confusion Matrix to have a look at 
the accuracy of these models.

###1.- PREDICTION WITH DECISION TREES

#### Fit the model:
```{r}
set.seed(12345)
library(rpart)
library(rpart.plot)
library(rattle)
modDC <- rpart(classe ~ ., data=trainclean, method="class")
fancyRpartPlot(modDC)
```

#### Prediction on Test data set (testclean2):
```{r}
predictionDC <- predict(modDC, testclean2, type = "class")
confusionMatrix(predictionDC, testclean2$classe)
conMatrixDC<-confusionMatrix(predictionDC, testclean2$classe)
```
#### PLOT MATRIX RESULTS:
```{r}
plot(conMatrixDC$table, col = conMatrixDC$byClass, 
     main = paste("DECISION TREES-ACCURACY =",
                  round(conMatrixDC$overall['Accuracy'], 4)))
```

###2.-PREDICTION USING RANDOM FORESTS

#### Fit the model:
```{r}
set.seed(12345)
ctrRF <- trainControl(method="cv", number=3, verboseIter=FALSE)
modRF <- train(classe ~ ., data=trainclean, method="rf", trControl=ctrRF)                       
modRF$finalModel
```

#### Prediction on Test data set (testclean2):
```{r}
predictionRF <- predict(modRF, newdata=testclean2)
confusionMatrix(predictionRF, testclean2$classe)
conMatrixRF <-confusionMatrix(predictionRF, testclean2$classe)
```
#### PLOT MATRIX RESULTS:
```{r}
plot(conMatrixRF$table, col = conMatrixRF$byClass, 
     main = paste("RANDOM FORESTS-ACCURACY =",
                  round(conMatrixRF$overall['Accuracy'], 4)))
```

###3.- PREDICTION USING GENERALIZED BOOSTED MODEL

#### Fit the model:
```{r}
set.seed(12345)
ctrGBM<- trainControl(method = "repeatedcv", number = 5, repeats = 1)
modGBM  <- train(classe ~ ., data=trainclean, method = "gbm",
                    trControl = ctrGBM, verbose = FALSE)
modGBM$finalModel
```

#### Prediction on Test data set (testclean2):
```{r}
predictionGBM <- predict(modGBM, newdata=testclean2)
confusionMatrix(predictionGBM, testclean2$classe)
conMatrixGBM <-confusionMatrix(predictionGBM, testclean2$classe)
```
#### PLOT MATRIX RESULTS:
```{r}
plot(conMatrixGBM$table, col = conMatrixGBM$byClass, 
     main = paste("GBM-ACCURACY =", round(conMatrixGBM$overall['Accuracy'], 4)))
```

## APPLYING THE SELECTED MODEL TO THE VALIDATION DATA
```{r}
AccuracyModels<-data.frame(Model=c("DC", "RF", "GBM"),
Accuracy = rbind(conMatrixDC$overall[1], conMatrixRF$overall[1], conMatrixGBM$overall[1]))
print(AccuracyModels)
```

We can observe that Random Forest has a high accuracy (over 99.70%) and this is the higher of all of these models; 
cross validation is done with K=3 and the expected out -of-sample error is less than 0.3%. Therefore, I will apply 
the Random Forest method to the validation data set (validationf) to predict the 20 test cases: 

#### Results (validation dataset):
```{r}
predictionVAL <- predict(modRF, newdata=validationf)
predictionVAL
```

#### Write the results to a text file for submission:
```{r}
pml_write_files = function(x) {
        n = length(x)
        for(i in 1:n){
                filename = paste0("problem_id_",i,".txt")
                write.table(x[i], file = filename, quote = FALSE, row.names = FALSE, col.names = FALSE)
        }
}

pml_write_files(predictionVAL)
```

## CONCLUSION 

Using exploratory analysis and combining different statistical models, our analysis suggests that our prediction 
function, developed using the Random Forests method with cross-validation, is be able to have a high accuracy 
(over 99.70%) to predict the 20 test cases (the manner in which the participants did the exercise) with 100% accuracy 
(20 points were awarded after submitting the 20 .txt files on the Course Project Submission). Random Forests is the 
more accurate method for our analysis after comparing it with other different methods such as Decision Trees and 
Generalized Boosted Model.

